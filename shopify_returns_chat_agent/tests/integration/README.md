# ðŸ”§ Integration & Performance Testing Suite

This comprehensive testing suite validates the complete system integration and performance of the Shopify Returns Chat Agent under production-like conditions.

## ðŸ“‹ Overview

The integration testing suite includes:

- **System Integration Tests** - End-to-end API functionality
- **Performance Tests** - Response times, concurrent users, load testing
- **Database Performance** - Data persistence and retrieval under load
- **Error Handling** - System resilience and recovery
- **Frontend-Backend Integration** - Complete widget-to-API flow

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
# From the integration test directory
pip install -r requirements.txt
```

### 2. Set Environment Variables

```bash
# Required
export API_BASE_URL="https://returnbot-production.up.railway.app"

# Optional test data
export TEST_ORDER_ID="4321"
export TEST_CUSTOMER_EMAIL="test@example.com"

# Optional debugging
export RETURNS_DEBUG="true"
```

### 3. Run All Tests

```bash
# Run complete test suite
python run_integration_tests.py --all

# Run only integration tests
python run_integration_tests.py --integration-only

# Run only performance tests  
python run_integration_tests.py --performance-only
```

## ðŸ“Š Test Categories

### System Integration Tests (`test_system_integration.py`)

**TestSystemIntegration:**
- âœ… Health endpoint validation
- âœ… Conversation start/chat flow  
- âœ… Complete return journey simulation
- âœ… Invalid input handling
- âœ… Concurrent conversation management
- âœ… Session persistence
- âœ… CORS headers verification

**TestShopifyIntegration:**
- âœ… Order lookup simulation
- âœ… Customer email verification flow
- âœ… Return policy validation

**TestDatabaseOperations:**
- âœ… Conversation logging
- âœ… State retrieval and persistence
- âœ… Data consistency across sessions

**TestSystemResilience:**
- âœ… Rate limiting handling
- âœ… Large message processing
- âœ… Special character/Unicode support

### Performance Tests (`test_performance.py`)

**TestResponseTime:**
- âš¡ Single request latency measurement
- âš¡ Conversation initialization speed
- âš¡ Sequential request consistency

**TestConcurrentUsers:**
- ðŸ‘¥ Multiple users on same endpoint
- ðŸ‘¥ Concurrent conversation handling
- ðŸ‘¥ Session isolation verification

**TestLoadTesting:**
- ðŸ“ˆ Incremental load (1-10 RPS)
- ðŸ’¥ Burst traffic handling
- ðŸ“Š Performance degradation analysis

**TestDatabasePerformance:**
- ðŸ’¾ Conversation storage efficiency
- ðŸ’¾ Concurrent data operations
- ðŸ’¾ Query performance optimization

**TestResourceUsage:**
- ðŸ§  Memory usage stability
- ðŸ”— Connection handling
- ðŸ“¦ Resource cleanup verification

**TestEndToEndPerformance:**
- ðŸ›’ Complete return journey timing
- ðŸ”„ Multi-step conversation analysis

## ðŸ“ˆ Performance Thresholds

The tests validate against these performance standards:

| Metric | Threshold | Description |
|--------|-----------|-------------|
| **Average Response Time** | < 2.0s | Mean API response time |
| **95th Percentile Response** | < 5.0s | P95 response latency |
| **Success Rate** | â‰¥ 95% | Successful request percentage |
| **Concurrent Users** | â‰¤ 10 | Max simultaneous users tested |

## ðŸŽ¯ Test Execution Options

### Individual Test Files

```bash
# Run system integration tests only
pytest test_system_integration.py -v

# Run performance tests only  
pytest test_performance.py -v -s

# Run specific test class
pytest test_system_integration.py::TestSystemIntegration -v

# Run specific test method
pytest test_performance.py::TestResponseTime::test_single_request_response_time -v
```

### Test Runner Script

```bash
# Complete test suite with reports
python run_integration_tests.py --all

# Test specific API endpoint
python run_integration_tests.py --api-url "http://localhost:8000"

# Generate reports from existing results
python run_integration_tests.py --generate-report

# Use custom configuration
python run_integration_tests.py --config-file custom_config.json
```

## ðŸ“Š Report Generation

The test runner generates comprehensive reports in multiple formats:

### 1. **JSON Report** (`test_results/reports/integration_report_YYYYMMDD_HHMMSS.json`)
- Machine-readable test results
- Detailed performance metrics
- Complete test execution data

### 2. **HTML Report** (`test_results/reports/integration_report_YYYYMMDD_HHMMSS.html`)
- Visual dashboard with charts
- Performance trend analysis
- Recommendation summaries

### 3. **Console Report**
- Real-time test progress
- Summary statistics
- Immediate feedback

## ðŸ”§ Configuration

### Default Configuration

```json
{
  "api_url": "https://returnbot-production.up.railway.app",
  "test_timeout": 300,
  "performance_thresholds": {
    "response_time_avg": 2.0,
    "response_time_p95": 5.0,
    "success_rate_min": 95.0,
    "concurrent_users_max": 10
  },
  "test_data": {
    "order_id": "4321",
    "customer_email": "test@example.com"
  },
  "report_formats": ["json", "html", "console"]
}
```

### Custom Configuration

Create `custom_config.json`:

```json
{
  "api_url": "https://your-custom-api.com",
  "performance_thresholds": {
    "response_time_avg": 1.5,
    "success_rate_min": 98.0
  },
  "test_data": {
    "order_id": "12345",
    "customer_email": "customer@yourstore.com"
  }
}
```

## ðŸ› Debugging and Troubleshooting

### Common Issues

**1. API Connection Failures**
```bash
# Check API health
curl https://returnbot-production.up.railway.app/health

# Test with localhost
python run_integration_tests.py --api-url "http://localhost:8000"
```

**2. Performance Test Failures**
```bash
# Run with verbose output
pytest test_performance.py -v -s --tb=long

# Check system resources
python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%, Memory: {psutil.virtual_memory().percent}%')"
```

**3. Environment Setup Issues**
```bash
# Verify environment variables
echo $API_BASE_URL
echo $TEST_ORDER_ID

# Install missing dependencies
pip install -r requirements.txt
```

### Debug Mode

Enable debug mode for detailed output:

```bash
export RETURNS_DEBUG="true"
python run_integration_tests.py --all
```

## ðŸ“ File Structure

```
tests/integration/
â”œâ”€â”€ __init__.py                     # Package initialization
â”œâ”€â”€ conftest.py                     # Pytest fixtures and configuration
â”œâ”€â”€ test_system_integration.py     # System integration tests
â”œâ”€â”€ test_performance.py            # Performance and load tests
â”œâ”€â”€ run_integration_tests.py       # Comprehensive test runner
â”œâ”€â”€ requirements.txt               # Test dependencies
â”œâ”€â”€ README.md                      # This documentation
â””â”€â”€ test_results/                  # Generated reports and results
    â”œâ”€â”€ reports/                   # HTML and JSON reports
    â”œâ”€â”€ logs/                      # Test execution logs
    â”œâ”€â”€ integration_results.json   # Latest integration results
    â””â”€â”€ performance_results.json   # Latest performance results
```

## ðŸŽ¯ Best Practices

### Before Running Tests

1. **Verify API is Running**
   ```bash
   curl -f https://returnbot-production.up.railway.app/health
   ```

2. **Check System Resources**
   - Ensure adequate memory (â‰¥ 4GB available)
   - Stable network connection
   - No competing high-CPU processes

3. **Environment Setup**
   - Set all required environment variables
   - Use test-specific configuration
   - Clear any cached test data

### During Testing

1. **Monitor Progress**
   - Watch console output for real-time metrics
   - Check for memory/CPU usage spikes
   - Note any unusual response times

2. **Performance Baselines**
   - Compare results against previous runs
   - Look for performance regressions
   - Document any environmental changes

### After Testing

1. **Review Reports**
   - Check all generated report formats
   - Analyze performance trends
   - Review failed test details

2. **Action Items**
   - Address any failed tests
   - Optimize performance bottlenecks
   - Update thresholds if necessary

## ðŸ”® Advanced Usage

### Custom Test Scenarios

Add new test scenarios by extending the existing test classes:

```python
# In test_system_integration.py
class TestCustomScenarios:
    def test_your_custom_scenario(self, api_client, conversation_id):
        # Your custom test logic
        pass
```

### Performance Benchmarking

Create performance benchmarks for specific scenarios:

```python
# In test_performance.py  
def test_benchmark_specific_flow(self, api_client, performance_tracker):
    # Benchmark specific conversation flows
    pass
```

### CI/CD Integration

```yaml
# Example GitHub Actions integration
- name: Run Integration Tests
  run: |
    cd shopify_returns_chat_agent/tests/integration
    pip install -r requirements.txt
    python run_integration_tests.py --all
  env:
    API_BASE_URL: ${{ secrets.API_BASE_URL }}
```

## ðŸ“ž Support

For issues with the integration testing suite:

1. Check the generated reports for detailed error information
2. Review the troubleshooting section above
3. Verify your environment configuration
4. Ensure the API is accessible and healthy

## ðŸ† Success Criteria

The integration testing suite considers the system ready for production when:

- âœ… All integration tests pass (100% success rate)
- âœ… Performance metrics meet defined thresholds
- âœ… No critical errors in error handling tests
- âœ… Concurrent user tests complete successfully
- âœ… Database operations perform within limits
- âœ… End-to-end journeys complete efficiently

---

*Generated by Shopify Returns Chat Agent Integration Testing Suite v1.0* 